{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b42e37",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461b75c",
   "metadata": {},
   "source": [
    "Sistema diseñado para procesar datos de manera distribuida sobre clùsters. Spark puede\n",
    "procesar cantidades de datos en el orden de terabytes incluso petabytes: El concepto de Spark es\n",
    "imaginar un cluster como una memoria gigante, la memoria resultante de combinar las memorias\n",
    "de todos los clusters. Se prioriza el uso de memoria y consigue ser muy rápido el procesamiento\n",
    "de la información, mayor que si utilizara MapReduce (Google). Spark utiliza MapReduce para\n",
    "algunas tareas de clasificación mediate regresión. Apache Spark está implementado en Scala\n",
    "que es ejecutado en la máquina virtual de java. Además de Spark ofrece interfaces de\n",
    "programación para java, python y R.\n",
    "Caracteristicas: velocidad de procesamiento, soporte multilenguajes, análisis avanzado Se puede\n",
    "desarrollar en tres maneras: 1) solo: standalone. Utiliza como base HDFS y encima se encuentra\n",
    "spark. 2) Hadoop (Yarn, administrador de recursos) : Base HDFS -> Yar/Mesos(kernel\n",
    "administrador del cluster) -> Spark 3) Spark con MapReduce (Spark In MapReduce, SIMR) : Base\n",
    "HDFS ->MapReduce y dentro se encuentra Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecff5c6",
   "metadata": {},
   "source": [
    "driver: proceso principal. Este proceso tiene un objeto SparkContext que le permite conectar con\n",
    "el gestor del cluster y reservar procesos executor en los distintos nodos del cluster:. Cada uno de\n",
    "los nodos del cluster (tambien se conocen como worker) podrá ejecutar uno o varios procesos\n",
    "executor que almacenará fragmentos de los datos del programa y realizarà operaciones sobre\n",
    "ellos: Durante la ejecución irá enviando peticiones a los distintos procesos executor que pueden\n",
    "contactar entre ellos para realizar tareas y comunicar con el proceso driver para devolver\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a263166",
   "metadata": {},
   "source": [
    "RDD (Resilient Distributed Datasets) Tipo de datos básico. Estos datos almacenan información de\n",
    "manera distribuida entre todos los equipos del cluster. Durante la ejecución de un programa Spark\n",
    "se construyen varios DDs que se dividen en distintos fragmentos y son almacenados en la\n",
    "memoria de los equipos del cluster. Caracteristica: 1) Están formados por un conjunto de\n",
    "registros, también llamados elementos, todos del mismo tipo. Por ejemplo, si cargamos un ROD\n",
    "apartir de un archivo plano se crear n RDD de cadenas de texo, una por cada linea del archivo.\n",
    "En Scala o Java, al declarar un DD se debe definir el tipo de los registros: Lenguajes estáticos\n",
    "RDD{String] en Scala y JavaRDD en Java. En dinámicos : python se pueden mezciar los tipos de\n",
    "datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398cee6a",
   "metadata": {},
   "source": [
    "RDDs han sido diseñados desde el inicio para ser distribuidos: los registros que lo componen se\n",
    "repartiran entre los clúster. Para realizar esta distribución: los DDs se dividen en particiones.\n",
    "Cada partición se almacena únicamente en un proceso executor dentro de un nodo del clúster,\n",
    "aunque un proceso executor puede albergar distintas particiones de distintos DDs. El número de\n",
    "particiones en las que dividir un DD se puede configurar e incluso cambiar a lo largo de la\n",
    "ejecución, por default es el número de núcleos de procesamiento disponibles en el cluster.\n",
    "Para decidir qué registros forman parte de cada partición, Spark utiliza particionadores, que son\n",
    "funciones que toman un registro y devuelven el número de la partición a la que pertenecen. Estos\n",
    "particionadores se puede configurar si se desea mejorar el rendimiento o dejar el default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f2edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a61586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b87174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName = \"Prueba\")  #sc objeto que apunta a SC al cluster local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadbba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c10b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando un RDD de 5 enteris: se crea en la memoria dell proceso del driver\n",
    "r = sc.parallelize([1,2,3,4,5])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.collect()  #Toma el rdd y lo colecciona y se ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bac421",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4258a",
   "metadata": {},
   "source": [
    "RDDs admiten dos tipos de operaciones:\n",
    "transformaciones y acciones\n",
    "pero ninguna de ellas modifica el RDD\n",
    "*Transformaciones:\n",
    "son operaciones que toman un RDD de partida y crean un nuevo\n",
    "RDD, dejando el original intacto.\n",
    "Ejemplos: son aplicar una\n",
    "función a todos los registros del RDD\n",
    "(por ejemplo, sumar una cierta cantidad),\n",
    "filtrar únicamente aquellos registros que cumplan una cierta condición i\n",
    "mediante algún campo.\n",
    "*Acciones: son operaciones que realizan algún cómputo sobre\n",
    "el RDD V devuelven un valor, dejando también el RDD original inalterado,\n",
    "Ejemplos: sumar todos los elementos almacenados\n",
    "en un RDD de números, generando un valor final, \"vaciar\" un RDD a un archivo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaea09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando un RDD de 3 cadenas de texto\n",
    "r = sc.parallelize([\"hola\",\"hi\",\"ciao\"])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.collect()\n",
    "\n",
    "# Salida\n",
    "# ['hola', 'hi', 'ciao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al Crear RDDs a partir de colecciones en memoria del proceso driver\n",
    "#estamos lomitados a la memoria que este proceso tenga disponible\n",
    "#Por eso, es mejor generar RDDs a partir de archivos.\n",
    "#Spark proporciona el metodo textFile, que carga el arch de texto y \n",
    "#genera  un RDD de cadenas de texto\n",
    "#En un sistema distribuido:\n",
    "#\"hdfs://node:poprt/data/file.txt, Amazon S3 \" s3n://bucket/file.txt\n",
    "#Spark cuenta con metodos para abrir archivos en formato\n",
    "#hadoop sc.hadoopFile\n",
    "r = sc.textFile(\"D:\\\\Universidad\\\\Semestre_8\\\\\\Big Data\\\\Datasets-20220222\\\\titanic.csv\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a45b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.collect(),\"\\n\")  #Pasa los renglones a un solo elemnto de la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acciones (Se ejecutan inmediatamente en todo el cluster)\n",
    "#son operaciones que realizan un procesamiento\n",
    "#sobre todo un RDD y devuelven un valor, dejando el RDD original,\n",
    "#en el mismo estado (es inmutable).\n",
    "#El valor generado se envia al proceso driver\n",
    "#Un RDD puede almacenar gigabytes, terabytes o inlcuso pertabytes\n",
    "#de datos de manera distribuida pero el valor generado\n",
    "#por las acciones se regresara el proceso driver. En este caso,\n",
    "#tomar en cuenta la memoria del proceso driver\n",
    "#por las acciones se ejecutan de manera inmediata  a diferencia de las transformaciones\n",
    "#Método collect : recorre el RDD y devuelve una lista, RDDs pequeños\n",
    "r = sc.parallelize([1,2,3,4,5,6])\n",
    "print(type(r.collect()))\n",
    "r.collect()\n",
    "#Se greneran 6 elementos en el cluster, collect busca los elementos en el cluster\n",
    "# y devuelve la informacion al proceso driver\n",
    "\n",
    "# Salida\n",
    "# <class 'list'>\n",
    "# [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe28993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para muchos elementos en el cluster se utiliza take que devuelve los primeros elementos encontrados al driver, no muestra todos\n",
    "#sc.setLogLevel(\"WARN\")\n",
    "r = sc.parallelize(range(1000))\n",
    "r.take(5)\n",
    "#sc.paralellize(range(1000)).cache().take(5)\n",
    "\n",
    "# Salida\n",
    "# [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce711ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuenta los elementos del RDD\n",
    "r.count()\n",
    "\n",
    "# Salida\n",
    "# 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af96c3a",
   "metadata": {},
   "source": [
    "Concepto importante del RDD: Resilencia\n",
    "Tienen la capacidad de recuperar su estado inicial\n",
    "cuando existe algún problema.\n",
    "Esto se debe a que los RDDs son particionados y cada\n",
    "partición ha sido almacenada en un proceso executor\n",
    "Por lo que apartir de su estado inicial,\n",
    "repite las\n",
    "transformaciónes que tiene programadas. De esta manera\n",
    "regenera las particiones perdidas.\n",
    "Dos manera de trabajar con RDD:\n",
    "* paralelizando una coleccion que ya existe en el driver\n",
    "referenciando un dataset\n",
    "Referencia de la clase DD : https://spark.apache.org/does/1.1.1/api/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para generar un RDD con los mumeros del 1 al 5 y calcular su suma. definiremos una funcion de reduccion\n",
    "#add e invocariamos a reduce:\n",
    "def add(x, y):\n",
    "    return x+y\n",
    "\n",
    "#Llamada a reduce\n",
    "r = sc. parallelize(range(1,6))\n",
    "r.reduce(add)\n",
    "\n",
    "# Salida\n",
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando una funcion anónima o lambda\n",
    "r= sc.parallelize(range(1,6))\n",
    "r.reduce(lambda x,y : x+y)\n",
    "\n",
    "# Salida\n",
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metodo takeOrdered, take()\n",
    "r= sc.parallelize(range(1000))\n",
    "r.takeOrdered(10, lambda x: -x)\n",
    "\n",
    "# Salida\n",
    "# [910, 919, 782, 534, 5, 862, 106, 562, 819, 182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efeadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.takeSample(False,10)\n",
    "\n",
    "# Salida\n",
    "# [999, 998, 997, 996, 995, 994, 993, 992, 991, 990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea31aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_positive(x,y):\n",
    "    if x > 0 and y > 0:\n",
    "        return x*y\n",
    "    elif x > 0:\n",
    "        return x\n",
    "    elif y > 0:\n",
    "        return y\n",
    "    else:\n",
    "        return 1\n",
    "#Multiplica solo los valores positivos  agarrando pares de valores desde el inicio de la lista  \n",
    "r = sc.parallelize([-1,2,1,-5,8])\n",
    "r.reduce(multiply_positive)\n",
    "\n",
    "# Salida\n",
    "# 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93701ee",
   "metadata": {},
   "source": [
    "Una de las acciones más útiles sobre DDs es reduce permite recorrer todos los valores de un\n",
    "RDD y calcular un valor en relación con ellos:. Reduce un RDD a un único valor. Dado un RDD\n",
    "con elementos de tipo T, la función de reducción es una función binaria que acepta dos elementos\n",
    "de tipo T y devuelve un valor del mismo tipo T, es decir, tiene tipo TX T -> T. reduce(f) es un\n",
    "método que aplica la función de reducción f a los 2 primeros elementos, luego aplica de nuevo la\n",
    "función f al valor resultante y al tercer elemento, y asi sucesivamente hasta que procesa el ùltimo\n",
    "elemento y produce el valor final. Ejemplo: Se muestra el proceso para calcular la suma de un\n",
    "RDD de 5 elementos enteros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fcb579",
   "metadata": {},
   "source": [
    "reduce\n",
    "se require que la función sea conmutativa y asociativa\n",
    "para los dos valores cualquiera x,y, f(x,y) = F(y, x)\n",
    "y para una triada : x,y,z, f(x,F(y,z) ) = f(F(x,y), Z).\n",
    "Ejemplos de funciones conmutativas y asociativas son la suma,\n",
    "la multiplicación, el minimo o el máximo.\n",
    "La resta o la división no son conmutativas ni asociativas.\n",
    "(no se pueden reducir)\n",
    "* MapReduce require de acciones como la replicación,\n",
    "\"serialización y E/S de almacenamiento para poder ejecutar\n",
    "* las operaciones solicitadas.\n",
    "Estas últmas son las que más requiere el sistema de archivos de Hadoop\n",
    "* Cualquier operación de MapReduce require del almacenamiento.\n",
    "reduce es una Acción en el RDD.\n",
    "Acciones: collect(), count(), take(x), takeordered (x,y),\n",
    "takeSample(x, y), countByKey (), (foreach (funcion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo\n",
    "r = sc.parallelize (range(3),1) #Una particion [0,1,2]\n",
    "r.reduce(lambda x,y : x-y)\n",
    "#(0-1)-2\n",
    "\n",
    "# Salida\n",
    "# -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71043704",
   "metadata": {},
   "outputs": [],
   "source": [
    "r= sc.parallelize(range(3),2) #Dos particiones[0], [1,2]\n",
    "r.reduce(lambda x,y : x-y)\n",
    "\n",
    "# Salida\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de Cadenas. Utilizacion del metodo aggregate\n",
    "# Tres parametros:\n",
    "#  1) un valor inicial para el acumulador, zeroValue que tendra tipoC\n",
    "#  2) Una funcion seqOp para combinar elementyos de nuestro RDDd (del tipo 1)\n",
    "#    con el acumulador de tipo C, devolviendo un valor de tipo C ( C x T -> C)\n",
    "#  3) Una funcion combOp para combinar dos acumuladores de tipo C\n",
    "#    y devolver un valor de tipo c\n",
    "# Cuenta las h en el RDD\n",
    "r = sc.parallelize([\"hola\",\"hi\",\"ciao\"])\n",
    "#            (int, cuenta las \"h\", suma los acumuladores) -> (valor inicial,map, reduce)\n",
    "#                 map                               Reduce\n",
    "r.aggregate(0,lambda c, s : c + s.count(\"h\"), lambda c1, c2: c1 +c2)\n",
    "#(h,1)\n",
    "#(h,1)\n",
    "#(h,0)\n",
    "\n",
    "# Salida\n",
    "#  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fae83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para trabajo de archivos en HDP , Consultar clase pyspark.RDD\n",
    "#saveAsSequenceFile, saveAsNewAPIHadoopDataset o saveAsNewAPIHadoopFile\n",
    "r = sc.parallelize(range(100),2)\n",
    "r.saveAsTextFile(\"C:/Users/eguer/Documents/Big_Data/numeros\")\n",
    "#Crea una carpeta con los archivos resultantes del RDD.\n",
    "#ElRDD se particionará en varios procesos executor, al convertir\n",
    "#un RDD a texto cada particion generará un archivo diferente,\n",
    "#Cada archivo_part-XXXX (XXX numeracion correlativa)\n",
    "#proceso de grabacion fue exitosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9b294",
   "metadata": {},
   "source": [
    "Transformaciones Son operaciones sobre los RDD complementarias a las acciones. Se\n",
    "caracterizan por tomar un RDD de origen y generar otro DD como resultado: El RDD resultante\n",
    "puede almacenar datos del mismo tipo. Generalmente diferente tipo del origen:. Una caracteristica\n",
    "importante: al ejecutar una transformación, ésta no realizarà ningún procesamiento real con el\n",
    "RDD, sólo \"anotará\" los datos de la transformación para ejecuaria en el futuro. Esto se debe a que\n",
    "Spark agrupa varias transformaciones y luego las ejecuta de manera agrupada, a esto se le\n",
    "conoce como tapa (stage). Las acciones \"son el disparo\" de la ejecución de las transformaciones\n",
    "pero sólo al punto de devolver una respuesta a la acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4075bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformacion map\n",
    "#Permite aplicar una funcion elemento a elemento en un RDD.\n",
    "#RDD con elementos tipo T y una funcion f que acepta elementos T\n",
    "#y produce valores tipo V, la transformacion\n",
    "#map(f) generara un RDD de el elementos tipo V\n",
    "r = sc.parallelize([1,2,3,4])   #A TODOS LOS ELEMENTOS LES SUMA 1\n",
    "r2 = r.map(lambda x: x+1)\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sin lambda\n",
    "def increment(x):\n",
    "    return x+1\n",
    "\n",
    "r = sc.parallelize([1,2,3,4])  \n",
    "r2 = r.map(increment)\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize([\"hola\",\"hi\",\"ciao\"]) \n",
    "r2 = r.map(lambda x: len(x))\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [4, 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fed024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "r = sc.parallelize([\"1,5,7\",\"8,2,4\"])   #Cadenas en formato CSV\n",
    "r2 = r.map(lambda x: list(csv.reader([x]))[0])\n",
    "#como resultado una lista/tupla. Una lista dentro de otra lista,\n",
    "#por eso se utiliza [0]\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [['1', '5', '7'], ['8', '2', '4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be371a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r2.map(lambda l: [int(e) for e in l])\n",
    "r3.collect()\n",
    "\n",
    "# Salida\n",
    "# [[1, 5, 7], [8, 2, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5306e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatmap : para aplanar las listas\n",
    "r = sc.parallelize([\"1,5,7\",\"8,2,4\"])\n",
    "r2 = r.flatMap(lambda s: list(csv.reader([s]))[0])\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# ['1', '5', '7', '8', '2', '4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter permite seleccionar de un RDD aquellos elementos que cumplen con una condicion\n",
    "\n",
    "def es_primo(x):\n",
    "    for i in range(2,x):\n",
    "        if x % i ==0:\n",
    "            return False\n",
    "        return True\n",
    "r = sc.parallelize(range(2,31))\n",
    "r2 = r.filter(es_primo)\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parejas (llave, valor)\n",
    "#mapValues permite aplicar una funcion a todos los registros pero afectando unicamente a los valores de las parejas\n",
    "r = sc.parallelize([(\"a\",0),(\"b\",1),(\"c\",2)])\n",
    "r2 = r.mapValues(lambda x: x+1)\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [('a', 1), ('b', 2), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considerar parejas (str, str) ca da valor es una de numeros en formato CSV\n",
    "import csv\n",
    "r = sc.parallelize([('a','1,5,7'),('b','8,2')])\n",
    "r2 = r.flatMapValues(lambda x: list(csv.reader([x]))[0])\n",
    "r2.collect()\n",
    "\n",
    "# Salida\n",
    "# [('a', '1'), ('a', '5'), ('a', '7'), ('b', '8'), ('b', '2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obejtos iterables con groupByKey\n",
    "r = sc.parallelize([('a', 3.14), ('b', 9.4), ('a', 2.7)])\n",
    "r2 = r.groupByKey()\n",
    "r2.collect()\n",
    "\n",
    "# Salida:\n",
    "# [('b', <pyspark.resultiterable.ResultIterable at 0x1ee6297e308>),\n",
    "#  ('a', <pyspark.resultiterable.ResultIterable at 0x1ee6297e2c8>)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar se aplica una transformación.\n",
    "\n",
    "r3 = r2.mapValues(lambda x: list(x))\n",
    "r3.collect()\n",
    "\n",
    "# Salida:\n",
    "# [('a', [3.14, 2.7]), ('b', [9.4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación reduceByKey Comportamiento parecido a groupByKey\n",
    "\n",
    "r = sc.parallelize([('a',2),('b',1),('a',3)])\n",
    "r2 = r.reduceByKey(lambda x,y: x+y)\n",
    "r2.collect()\n",
    "\n",
    "\n",
    "# Salida:\n",
    "# [('b',1), ('a',5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación combinando RDDs\n",
    "# union\n",
    "\n",
    "r1 = sc.parallelize([1,2,3,4])\n",
    "r2 = sc.parallelize([2,4,6])\n",
    "r3 = r1.union(r2)\n",
    "r3.collect()\n",
    "\n",
    "# Salida:\n",
    "# [1,2,3,4,2,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944afb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct\n",
    "\n",
    "r4 = r3.distinct()\n",
    "r4.collect()\n",
    "\n",
    "# Salida:\n",
    "# [1,2,3,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection\n",
    "\n",
    "r1 = sc.parallelize([1,2,3,4])\n",
    "r2 = sc.parallelize([2,4,2,6])\n",
    "r3 = r1.intersection(r2)\n",
    "r3.collect()\n",
    "\n",
    "# Salida:\n",
    "# [2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e25992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract: selecciona elementos de un RDD (r1) que no se encuentren\n",
    "# en otro (r2). Si en r1 aparecen repetidos, apareceran en el RDD\n",
    "# resultante\n",
    "\n",
    "r1 = sc.parallelize([1,2,3,4,1])\n",
    "r2 = sc.parallelize([2,6])\n",
    "r3 = r1.subtract(r2)\n",
    "r3.collect()\n",
    "\n",
    "# Salida\n",
    "# [1,1,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian\n",
    "\n",
    "r1 = sc.parallelize([1,2,3])\n",
    "r2 = sc.parallelize(['a','b'])\n",
    "r3 = r1.cartesian(r2)\n",
    "r3.collect()\n",
    "\n",
    "# Salida:\n",
    "# [(1,'a'),(1,'b'),(2,'a'),(2,'b'),(3,'a'),(3,'b')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join\n",
    "\n",
    "r1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "r2 = sc.parallelize([('b',8),('d',7)])\n",
    "r3 = r1.join(r2)\n",
    "r3.collect()\n",
    "\n",
    "# Salida:\n",
    "# [('b', (2,8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "r2 = sc.parallelize([('b',8),('d',7),('b',0)])\n",
    "r3 = r1.join(r2)\n",
    "r3.collect()\n",
    "\n",
    "# Salida:\n",
    "# [('b', (2,8)),('b', (2,0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pagina de guia:\n",
    "#https://spark.apache.org/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
